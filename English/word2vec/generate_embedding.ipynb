{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72279ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np\n",
    "import numpy.linalg as lin\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "import warnings \n",
    "stop_words = set(stopwords.words('english'))\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a305daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format('w2v.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c28a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8f52201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    regex = re.compile('[^a-zA-Z\\s]')\n",
    "    text_returned = re.sub(regex,' ',text)\n",
    "    return text_returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9696cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(word):\n",
    "    word=word.lower()\n",
    "    try:\n",
    "        return w2v.get_vector(word)\n",
    "    except:\n",
    "        return np.array([0.0]*300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af437599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence):\n",
    "    sentence=strip_accents(sentence)\n",
    "    sentence=remove_special_characters(sentence)\n",
    "    words=word_tokenize(sentence)\n",
    "    if len(words)>0:\n",
    "        words=[word  for word in words if word not in stop_words]\n",
    "        sentence_embedding=[word_embedding(word) for word in words]\n",
    "        return np.array(list(map(lambda x: sum(x)/len(x), zip(*sentence_embedding))))\n",
    "    return np.array([0]*300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd1ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('../English.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5da909b",
   "metadata": {},
   "source": [
    "# Sentence By Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa58543",
   "metadata": {},
   "outputs": [],
   "source": [
    "verse_embeddings_sentence=[]\n",
    "verse_embeddings_max=[]\n",
    "verse_embeddings_mean=[]\n",
    "for i in range(len(data)):\n",
    "    text=data.loc[i]['Commentary']\n",
    "    text=strip_accents(text)\n",
    "    sentences=sent_tokenize(text)\n",
    "    embeddings=[sentence_embedding(sentence) for sentence in sentences]\n",
    "    #sentence\n",
    "    verse_embeddings_sentence.append(embeddings)\n",
    "    #Max Pooling\n",
    "    norms=[lin.norm(i) for i in embeddings]\n",
    "    index=norms.index(max(norms))\n",
    "    verse_embeddings_max.append(embeddings[index])\n",
    "    #Mean Pooling\n",
    "    embeddings=np.array(list(map(lambda x: sum(x)/len(x), zip(*embeddings))))\n",
    "    verse_embeddings_mean.append(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73bed3",
   "metadata": {},
   "source": [
    "# Whole Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "888c27a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "verse_embeddings_whole=[]\n",
    "for i in range(len(data)):\n",
    "    text=data.loc[i]['Commentary']\n",
    "    sentence=strip_accents(text)\n",
    "    embeddings=sentence_embedding(sentence)\n",
    "    verse_embeddings_whole.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b01ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('sentence.pkl','wb')\n",
    "pickle.dump(verse_embeddings_sentence,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc743362",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('whole.pkl','wb')\n",
    "pickle.dump(verse_embeddings_whole,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ff339da",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('max.pkl','wb')\n",
    "pickle.dump(verse_embeddings_max,file)\n",
    "file.close()\n",
    "file=open('mean.pkl','wb')\n",
    "pickle.dump(verse_embeddings_mean,file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
